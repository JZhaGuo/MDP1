---
title: "Aprendizaje Supervisado y Análisis Discriminante"
author: "Sonia Tarazona"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: false
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(caret)
# Librería para validación de métodos supervisados
# https://cran.r-project.org/web/packages/caret/vignettes/caret.html
# http://topepo.github.io/caret/index.html
library(MASS) # Para el análisis discriminante
```


# APRENDIZAJE SUPERVISADO

## Predicciones de salario

A la hora de realizar algoritmos de aprendizaje automatizado sobre nuestra base de datos de Jugadores de beisbol, primero debemos definir una variable respuesta. En nuestro caso, vamos a tomar la variable salario como esta variable respuesta, cambiando sus valores a categorías (Salario alto o salario bajo) de forma que podamos clasificar a los jugadores en dichas categorías basandonos en su rendimiento en el campo.

```{r datosBeisbol}
beisbolOG = read.csv("beisbol.csv", row.names = 1, as.is = TRUE)
summary(beisbolOG)
```

Como podemos ver, la media de la variable Salary es 516280, por lo que vamos a separar en dos grupos: menos que la media, y más que la media. Estas agrupaciones son tentativas para realizar nuestro primer análisis.

```{r}

SalaryCat <- as.factor(ifelse(beisbolOG$Salary < 516280, '0',
                          ifelse(beisbolOG$Salary > 516280, '1', 0)))

beisbol <- beisbolOG[,-c(17)] # Quitamos la columna Salary para no estudiarla en la predicción del propio salary lol lmao

beisbol <- data.frame(beisbol, SalaryCat)

ttt = table(beisbol$SalaryCat)
kable(ttt)
kable(100*ttt/sum(ttt))


```

```{r percent}

beisbol
```



Como vemos, hemos dividido en las categorías acorde a la media, y observamos que hay una división 60/40 entre los que tienen un salario bajo y alto, respectivamente. Una vez hemos hecho esto, procedemos a separar los datos de entrenamiento de los datos de validación.

```{r percent}
set.seed(100)

trainFilas = createDataPartition(beisbol$SalaryCat, p=0.8, list=FALSE) # Números de las filas
trainDatos = beisbol[trainFilas,] # Separación de las filas de entrenamiento
testDatos = beisbol[-trainFilas,] # Separación de las filas que no son de entrenamiento (test)

num = table(trainDatos$SalaryCat)
perc = 100*num/sum(num)
kable(cbind(num, perc))
myTrainControl = trainControl(method = "repeatedcv",  # k-fold
                              number = 10,  # num folds
                              repeats = 30) # num veces a repetir CV


```



A continuación, vamos a generar un modelo de análisis discriminante que nos permita clasificar a los jugadores en las dos categorías de salario.

Para ello, obtendremos la FDL a partir de los datos de entrenamiento mediante el esquema de validación cruzada previamente definido. Aunque en este caso no vamos a optimizar ningún hiperparámetro, sí que nos permitirá estimar el error de clasificación en los datos de entrenamiento de forma más fidedigna.

**Nota 1:** En LDA se obtienen los mismos resultados de clasificación
centrando y escalando las variables que sin centrarlas y escalarlas (mismas probabilidades a posteriori), pero escalar nos permitirá interpretar los resultados para identificar las variables más discriminantes.

**Nota 2:** En este ejemplo, caret está utilizando la función lda() de la librería MASS para hacer el análisis discriminante. Por tanto, se pueden incluir también en la función train() los argumentos propios de la función lda().


```{r cancer1}
set.seed(100)  ## Fijamos la semilla aleatoria para que los resultados de la CV sean los mismos en cada ejecución
trainDatosESC = trainDatos
trainDatosESC[,-25] = scale(trainDatos[,-25])  
modeloTR = train(SalaryCat ~ ., data = trainDatosESC, method='lda', 
                 trControl = myTrainControl)  
modeloTR
# modeloTR$method
modeloTR$finalModel
# modeloTR$finalModel$scaling  ## Coeficientes de las variables en FDL
modeloTR$results
dim(modeloTR$resample)
head(modeloTR$resample)
```


Por defecto, la librería *caret* nos devuelve la exactitud (*Accuracy*) y el coeficiente Kappa. Nos devuelve la media y desviación típica de estos indicadores para los 300 modelos entrenados en la CV aplicados sobre el *fold* que se deja fuera para validar. Podemos observar que son valores medianamente altos, sobre todo la accuracy. La kappa se queda en un 0'50.
Esto nos indica que el LDA permite clasificar los jugadores bien. 

Los resultados generados también nos devuelven la siguiente información:

* Las probabilidades a priori utilizadas son las observadas en la muestra. Si pensamos que realmente el porcentaje de tumores malignos en la realidad es diferente al de la muestra, deberíamos cambiar las probabilidades a priori.

* Los coeficientes (LD1) de la función discriminante nos permitirán seleccionar las variables más discriminantes a la hora de clasificar los tumores.



----------

*EJERCICIO 4*

*Investiga si se podría haber hecho el centrado y escalado de los datos con la propia función train() y cómo lo habríamos hecho. ¿Se obtienen los mismos resultados?*

El centrado y escalado sí se puede hacer dentro de la propia función train(). Los resultados sí son los mismos, puesto que son los mismos datos. 

----------


Como habíamos indicado anteriormente, vamos a obtener la matriz de confusión del modelo final aplicado a los datos de entrenamiento y a los datos test, así como otras medidas del error adicionales. 

Realmente, para evaluar la bondad de clasificación del modelo, sería más adecuado analizar la matriz de confusión obtenida a partir de los datos test. Sin embargo, es conveniente explorar primero qué pasa con los datos de entrenamiento ya que, si con ellos no se obtienen buenos resultados, debemos seguir perfeccionando nuestro modelo o buscando otros modelos mejores.

Para la matriz de confusión, siempre hay que definir que entendemos como casos positivos (P) o negativos (N). En este caso, como queremos un método para detectar los tumores malignos con la mayor precisión posible, elegiremos como positivos los salarios altos.

**Nota:** Los datos test siempre deben escalarse de la misma forma en la que se han escalado los datos de entrenamiento, es decir, utilizando la media y desviación típica de las variables sobre los datos de entrenamiento. Si realizamos el centrado y escalado dentro de la propia función utilizada para generar el modelo, no será necesario porque la función *predict* lo tendrá en cuenta. En nuestro caso, como centramos y escalamos los datos de entrenamiento fuera de la función con *scale*, es necesario que hagamos lo propio con los datos test.


```{r cancer2}
# Datos de entrenamiento
ajusteTR = predict(modeloTR, type = "raw")
head(ajusteTR)
caret::confusionMatrix(ajusteTR, factor(trainDatosESC$SalaryCat), positive = "1")

# Datos test
testDatosESC = testDatos
testDatosESC[,-25] = scale(testDatos[,-25], center = colMeans(trainDatos[,-25]), 
                     scale = apply(trainDatos[,-25], 2, sd))
ajusteTest = predict(modeloTR, testDatosESC, type = "raw") 
caret::confusionMatrix(ajusteTest, factor(testDatos$SalaryCat), positive = "1")
```

Como podemos observar en estos resultados, las medidas del error de clasificación son buenas tanto en los datos de entrenamiento como en los datos test. Podemos observar que todos los valores de accuracy se encuentran por encima de 0'75, mientras que la kappa tiene unos valores más bajos, de alrededor de 0'5.

Vemos que en ambos casos los valores de accuracy y kappa son bastante altos, y las medidas de sensibilidad y especificidad de los datos test también es alta. La más baja se encuentra en los valores de sensibilidad de los datos test, que es 0'6.



----------

*EJERCICIO 5*

*Crea una función que te permita calcular el coeficiente de correlación de Matthews (MCC) a partir de la matriz de confusión. Aplícala para calcular el MCC sobre los datos test del ejemplo anterior.*

Ver si se pueden sacar los datos de la matriz de confusión sin hardcodearlos. Luego aplicar la fórmula (diapositiva 11).

----------



Vamos a generar la curva ROC del modelo LDA para los datos test:

```{r beisbol2roc, warning=FALSE, message=FALSE}
library(pROC)
Y = testDatos$SalaryCat
ajusteTestProb = predict(modeloTR, testDatosESC, type = "prob") 
head(ajusteTestProb)
miroc = roc(Y ~ ajusteTestProb[,"1"], plot = TRUE, print.auc = TRUE, col = "red3") 
# Elementos de la lista que devuelve al función roc:
names(miroc)
```

Como podemos observar en el gráfico, el área bajo la curva (AUC) es cercana a 1, lo que muestra que el modelo LDA es muy buen clasificador para estos datos.

----------

*EJERCICIO 6*

*A partir de los resultados de la curva roc, ¿podrías calcular el índice de Youden y obtener el valor de la probabilidad de clasificación (p) para el que se maximiza el índica de Youden?*

Igual que el ejercicio anterior, si conseguimos sacar los datos se puede aplicar la fórmula de forma sencilla (diapositiva 10). Se interpreta como (-1, 1) y cuanto más cercano a 1, mejor clasificado (si es 0, es que los datos están clasificados aleatoriamente).

----------


Veamos la representación gráfica de las puntuaciones discriminantes sobre, por ejemplo, los datos de entrenamiento: 

```{r cancer3}
z = as.matrix(trainDatosESC[,-25]) %*% modeloTR$finalModel$scaling
plot(density(z[trainDatos$SalaryCat == "0"]), col = 4, xlab = "z",
     xlim = range(z), main = "Salario de los jugadores", lwd = 2)
lines(density(z[trainDatos$SalaryCat == "1"]), col = 2, lwd = 2)
abline(v = mean(z[trainDatos$SalaryCat == "0"]), col = 4, lty = 2, lwd = 2)
abline(v = mean(z[trainDatos$SalaryCat == "1"]), col = 2, lty = 2, lwd = 2)
```


Por último, vamos a seleccionar las variables que más han contribuido a clasificar los tumores. Para ello, evaluaremos sus coeficientes (en valor absoluto) en la FDL. Representaremos en sendos diagramas de cajas y bigotes, la diferencia entre los valores de la variable más discriminante y de una de las menos discriminantes entre los dos grupos de tumores. 

```{r cancer4}
myW = modeloTR$finalModel$scaling[,1]
barplot(sort(abs(myW), decreasing = TRUE), las = 2, cex.names = 0.5) 
par(mfrow = c(1,2))
boxplot(CHits ~ SalaryCat, data = trainDatos, col = "grey", notch = TRUE)
boxplot(Weight ~ SalaryCat, data = trainDatos, col = "grey", notch = TRUE)
```

Efectivamente, los gráficos de cajas y bigotes confirman cómo la primera variable (*CHits*) diferencia bastante bien los dos tipos de salarios, al contrario que la segunda variable (*Weight*).

Con esto podemos deducir que el salario se diferencia bien en función de los Hits a lo largo de la carrera que ha realizado un jugador, dado que los boxplot no se solapan. Por otro lado, el peso del jugador no es un factor determinante en el salario, puesto que los boxplot están casi completamente solapados.



----------

*EJERCICIO 7*

*Crea un gráfico de barras que represente las variables ordenadas según su poder discriminante pero que muestre los coeficientes con su signo correspondiente (no en valor absoluto). Colorea con distinto color las barras de coeficientes positivos o negativos.*

Si quitamos el valor absoluto de la variable myW a la hora de realizar el barplot:


```{r noabs}
variables = colnames(beisbol)[-25]


# Ordenar las variables y coeficientes de mayor a menor poder discriminante
orden <- order(abs(myW), decreasing = TRUE)
myW_ordenados <- myW[orden]
variables_ordenadas <- variables[orden]

# Vector de colores para coeficientes positivos y negativos
colores <- ifelse(myW_ordenados >= 0, "blue", "red")

# Crear el gráfico de barras con los coeficientes y sus signos correspondientes
barplot(myW_ordenados, names.arg = variables_ordenadas, col = colores, main = "Gráfico de Barras de Coeficientes",
        xlab = "Variables", ylab = "Coeficientes", las = 2, cex.names = 0.5)
```


## Análisis discriminante por liga y división.

Vamos a definir una nueva columna, donde agrupemos las categorías acorde a los años de experiencia del jugador, para ver si hay diferencias en su rendimiento durante la temporada del 86.

```{r ligadivision}

labels <- c("Rookie", "Junior", "Senior")

quantiles <- quantile(beisbolOG$Years, probs = seq(0,1,1/3))

beisbolOG$PlayerLevel <- cut(beisbolOG$Years, breaks = quantiles, labels = labels, include.lowest = TRUE)


beisbol2 <- beisbolOG

beisbol2["DIV_CAtBat"] = beisbol2["AtBat"]/beisbol2["CAtBat"] 
beisbol2["DIV_CHits"] = beisbol2["Hits"]/beisbol2["CHits"] 
beisbol2["DIV_CHmRun"] = beisbol2["HmRun"]/beisbol2["CHmRun"] 
beisbol2["DIV_CRuns"] = beisbol2["Runs"]/beisbol2["CRuns"] 
beisbol2["DIV_CRBI"] = beisbol2["RBI"]/beisbol2["CRBI"] 
beisbol2["DIV_CWalks"] = beisbol2["Walks"]/beisbol2["CWalks"]
# Cambiar para que 0 / 0 de 0 y no NaN

beisbol2 <- beisbol2[,-c(1:25)]

summary(beisbol2)

```
 Las clases que hemos creado están bastante equilibradas, por lo que procedemos a dividir los datos en los sets de entrenamiento y test, y los escalamos:

```{r traintestdiv}
set.seed(100)
trainFilas = createDataPartition(beisbol2$PlayerLevel, p=0.8, list=FALSE)
trainDatosW = beisbol2[trainFilas,]
testDatosW = beisbol2[-trainFilas,]
trainDatosWESC = trainDatosW
trainDatosWESC[,-1] = scale(trainDatosW[,-1], center = TRUE, scale = TRUE)
testDatosWESC = testDatosW
testDatosWESC[,-1] = scale(testDatosW[,-1], center = colMeans(trainDatosW[,-1]), 
                     scale = apply(trainDatosW[,-1], 2, sd))
```


Generamos el modelo lineal discriminante sobre los datos de entrenamiento y evaluamos su bondad de clasificación sobre estos datos y sobre los datos test. En esta ocasión, utilizaremos la función *lda* directamente para generar el modelo, en lugar de la librería caret, y no realizaremos validación cruzada sobre los datos de entrenamiento.

```{r vinos3}
modeloTR = lda(PlayerLevel ~ ., data = trainDatosWESC, CV = FALSE) 
modeloTR$prior
modeloTR$means
head(modeloTR$scaling) # coeficientes de la FDL
# Matriz de confusión para entrenamiento
ajusteTR = predict(modeloTR)
caret::confusionMatrix(ajusteTR$class, trainDatosW$PlayerLevel)
# Matriz de confusión para test
ajusteTest = predict(modeloTR, testDatosWESC)
caret::confusionMatrix(ajusteTest$class, testDatosW$PlayerLevel)
head(ajusteTest$posterior)
head(ajusteTest$x)  #  z (puntuaciones discriminantes)
```

Como podemos observar, al tener 3 grupos (3 variedades), se generan dos funciones discriminantes (LD1 y LD2).

La clasificación es perfecta, tanto para los datos de entrenamiento como para los datos test, ya que no hay ningún vino que esté mal clasificado.



Representaremos gráficamente las puntuaciones discriminantes (ahora en un gráfico de dos dimensiones) para todos los datos:

```{r vinos4}
WineE = rbind(trainDatosWESC, testDatosWESC)
plot.df <- data.frame(predict(modeloTR, WineE[,-1])$x, "Outcome" = WineE$Cultivar)
library(ggplot2)
ggplot(plot.df, aes(x = LD1, y = LD2, color = Outcome)) + geom_point()
```

Observamos cómo la primera función discriminante separa muy bien *barolo* y *barbera*, pero no tanto *grignolino*, que consigue clasificarse mejor gracias a la segunda función discriminante. Así pues, en este ejemplo, son necesarias ambas funciones discriminantes.


----------

*EJERCICIO 8*

*A partir de los resultados anteriores, ¿qué características de los vinos contribuyen a discriminar mejor cada variedad de uva?*

----------



# REGRESIÓN LOGÍSTICA

## Ejemplo 1: Cáncer de mama

Generamos un modelo de regresión logística para los datos de cáncer de mama. 

```{r reglog}
modeloTrRL = glm(diagnosis ~ ., data = trainDatos, family = "binomial")
summary(modeloTrRL)
```


Observamos que tenemos un problema en el ajuste del modelo de regresión logística. Veamos si puede ser debido a una alta correlación entre las variables predictoras que está dando lugar a un problema importante de multicolinealidad: 

```{r corre, warning=FALSE, message=FALSE}
library(corrplot)
corrplot(cor(trainDatos[,-1]), method = "ellipse", diag = FALSE, order = "hclust",
         hclust.method = "ward.D2", tl.col = 1, tl.cex = 0.8)
```


Efectivamente, tenemos un problema de multicolinealidad que afecta al ajuste del modelo de regresión logística. Ajustaremos otro modelo de regresión logística eliminando algunas de las variables más correlacionadas:


```{r reglog2}
modeloTrRL2 = glm(diagnosis ~ .-texture_peor-radius_m-perimeter_m-area_peor
                  -radius_peor-perimeter_peor-radius_se-perimeter_se
                  -concave_points_m-concavity_peor-concavity_se-
                    concavity_m-compactness_m-compactness_peor,
                  data = trainDatos, family = "binomial")
summary(modeloTrRL2)
```

Y por último, ajustaremos un modelo con las variables más significativas:

```{r reglog3}
modeloTrRL3 = glm(diagnosis ~ texture_m + area_m + fractal_dimension_m + area_se 
                  + compactness_se + fractal_dimension_se + concave_points_peor 
                  + symmetry_peor + fractal_dimension_peor,
                  data = trainDatos, family = "binomial")
summary(modeloTrRL3)
```

Comparamos, mediante curvas ROC, el modelo 3 de regresión logística, y el modelo LDA obtenido anteriormente:

```{r AUC, message=FALSE, warning=FALSE}
library(pROC)
miROC1 = roc(Y ~ ajusteTestProb[,"M"], plot = TRUE, print.auc = FALSE, col = "red3") 
ajusteTest3 = predict(modeloTrRL3, testDatos)
miROC3 = roc(testDatos$diagnosis ~ ajusteTest3, 
             plot = TRUE, col = "blue3", lty = 2, add = TRUE)
legend("bottomright", c(paste0("LDA - AUC = ", round(miROC1$auc,4)),
                        paste0("LR3 - AUC = ", round(miROC3$auc, 4))),
       col = c("red3", "blue3"), lwd = 2, bty = "n")
```

Ambos modelos tienen un AUC elevado, ligeramente mayor el del LDA que, además, no presenta ningún problema debido a la multicolinealidad en las variables predictoras.



# MANOVA

## Ejemplo 3: Vinos de Italia

En esta sección, generaremos un modelo MANOVA para los datos de vinos:

```{r manova}
Y = as.matrix(WineE[,-1])
fit = manova(Y ~ WineE$Cultivar)
summary(fit, test="Pillai")
summary.aov(fit)
```

Podemos observar que todas las variables contribuyen significativamente a discriminar entre las 3 variedades. Faltaría analizar qué variables presentan más diferencias entre qué variedades, información que extrajimos del LDA en el Ejercicio 8. 